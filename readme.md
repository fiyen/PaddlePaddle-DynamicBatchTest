# å°è¯•åŠ¨æ€BatchSizeæ•ˆæœ
ä¸åŒäºè®¡ç®—æœºè§†è§‰ä¸Šçš„å¤„ç†æ–¹å¼ï¼Œæ–‡æœ¬æ ·æœ¬ä¸€èˆ¬ä¸ºä¸€ä¸ªåºåˆ—ï¼Œè€Œå…¶å¯¹åº”çš„ç¼–ç é€šå¸¸æ˜¯ä¸ç­‰é•¿çš„ã€‚åœ¨æ„å»ºä¸€ä¸ªæ‰¹è¾“å…¥æ—¶ï¼Œé€šå¸¸æœ‰ä¸¤ç§å¤„ç†æ–¹å¼ï¼šä¸€ç§æ˜¯ç›´æ¥å°†æ‰€æœ‰æ ·æœ¬ç»™å¡«å……æˆç­‰é•¿çš„ç¼–ç åºåˆ—ï¼Œå³ç¼–ç åçš„è¾“å…¥é•¿åº¦éƒ½ç›¸ç­‰ï¼›å¦ä¸€ç§æ˜¯åœ¨ä¸€ä¸ªæ‰¹çš„æ•°æ®ä¸­å°†æ ·æœ¬å¡«å……æˆç­‰é•¿çš„ç¼–ç åºåˆ—ã€‚æ˜¾ç„¶ï¼Œç”±äºç¬¬ä¸€ç§æ–¹å¼å°†æ‰€æœ‰æ ·æœ¬éƒ½å¡«å……åˆ°æŒ‡å®šçš„é•¿åº¦ï¼Œä¸€äº›æœ¬æ¥æ¯”è¾ƒçŸ­çš„è¾“å…¥ä¼šå’Œé•¿åºåˆ—çš„è¾“å…¥ä¸€æ ·å ç”¨ç›¸åŒçš„æ˜¾å­˜ç©ºé—´ï¼Œè¿™ç§å¤„ç†æ–¹å¼æ˜¯ä¸ç»æµçš„ã€‚è€Œå¦ä¸€ç§æ–¹å¼ï¼Œå½“ä¸€ä¸ªæ‰¹ä¸­çš„æ‰€æœ‰æ ·æœ¬éƒ½å¾ˆçŸ­æ—¶ï¼Œæ‰¹é•¿åº¦ä¹Ÿå¾ˆçŸ­ï¼›å½“ä¸€ä¸ªæ‰¹ä¸­çš„æŸæ ·æœ¬å¾ˆé•¿æ—¶ï¼Œè¯¥æ‰¹çš„é•¿åº¦ä¼šå¾ˆé•¿ã€‚ç”±äºæ‰¹çš„æ ·æœ¬æ•°é‡é€šå¸¸æ˜¯å›ºå®šçš„ï¼Œå› æ­¤æ˜¾å­˜çš„å ç”¨å¯èƒ½å‡ºç°è¾ƒå¤§çš„æ³¢åŠ¨ã€‚æœ‰æ—¶åœ¨è®­ç»ƒä¸€ä¸ªæ¨¡å‹æ—¶ï¼Œåˆšå¼€å§‹è®­ç»ƒçš„å¥½å¥½çš„ï¼Œä¸­é€”çªç„¶å‡ºç°æ˜¾å­˜æº¢å‡ºï¼Œå¯èƒ½å°±æ˜¯è¿™ä¸ªåŸå› ã€‚ç”±äºæ˜¾å­˜çš„æ³¢åŠ¨ï¼Œèƒ½å¼€å§‹è®­ç»ƒä¸ä¸€å®šæ„å‘³ç€èƒ½è®­ç»ƒå®Œã€‚é‚£ä¹ˆï¼Œè‡ªç„¶ä¼šæœ‰äººé—®ï¼Œå¦‚æœå°†æ‰¹å¤§å°æ”¹ä¸ºåŠ¨æ€çš„ï¼Œå³æ‰¹çš„æ ·æœ¬æ•°é‡æ˜¯å¯å˜çš„ï¼Œæ‰¹é•¿åº¦é•¿åˆ™æ ·æœ¬æ•°å°‘ï¼Œæ‰¹é•¿åº¦çŸ­åˆ™æ ·æœ¬æ•°å¤šï¼Œè¿™æ ·æ˜¾å­˜çš„åˆ©ç”¨ç‡å°±ä¸ä¼šå‡ºç°æ³¢åŠ¨äº†ã€‚é‚£ä¹ˆè¿™æ ·å¤„ç†çš„æ–¹å¼ä¼šé€ æˆå…¶ä»–é—®é¢˜å—ï¼Ÿè¿™é‡Œæˆ‘ä»¬å°±è¿›è¡Œä¸€ä¸‹æ¢ç´¢ã€‚

**æœ¬äººçš„ç ”ç©¶æ–¹å‘æ˜¯åŸºäºå¤æ‚ç½‘ç»œçš„æ–‡æœ¬å¤„ç†æ–¹æ³•ï¼Œä¹Ÿåœ¨è‡´åŠ›äºæ¢ç©¶ä¸æ·±åº¦å­¦ä¹ ç›¸ç»“åˆçš„æ–¹æ³•ã€‚æˆ‘ä¼šä¸å®šæœŸæ›´æ–°è‡ªå·±çš„å·¥ä½œï¼Œå¦‚æœæœ‰ç ”ç©¶æ–¹å‘ç›¸åŒçš„ï¼Œæˆ–è€…æ„Ÿå…´è¶£çš„æœ‹å‹ï¼Œæ¬¢è¿ä¸‰è¿æ”¯æŒä¸€ä¸‹ã€‚[æ¥AI Studioäº’ç²‰å§~ç­‰ä½ å“¦~](https://aistudio.baidu.com/aistudio/personalcenter/thirdview/300157)**

### **æ›´å¤šé¡¹ç›®é“¾æ¥ç‚¹å‡»[æ²¡å…¥é—¨çš„ç ”ç©¶ç”Ÿçš„é¡¹ç›®åˆé›†](https://aistudio.baidu.com/aistudio/projectdetail/542430)**

## 1. å®šä¹‰åŠ¨æ€BatchSizeçš„æ•°æ®è¯»å–å™¨
ä¸ºäº†å®ç°è¿™ä¸ªæ“ä½œï¼Œæˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€ä¸‹æ”¯æŒåŠ¨æ€BatchSizeçš„æ•°æ®è¯»å–å™¨ã€‚æˆ‘ä»¬é‡æ–°å®šä¹‰ä¸€ä¸ªå‚æ•°controllerï¼Œæ¥è¡¨ç¤ºå¯¹batch_sizeå’Œinput_lengthçš„æ•´åˆã€‚æ˜¾å­˜çš„å ç”¨å¤§è‡´ç¬¦åˆ ç±»ä¼¼batch_size * input_length * input_lengthçš„å¢é•¿è§„å¾‹ï¼Œå› æ­¤æˆ‘ä»¬å°±å®šä¹‰ 
$controller=batch\_size*input\_length^2$

ä»¥ä¸‹å®šä¹‰å·¥å…·çš„å‚æ•°è§£é‡Šå¦‚ä¸‹ï¼š

data: å¯è¿­ä»£è¿”å›æ ·æœ¬çš„æ•°æ®ï¼Œå¯ä»¥æ˜¯list,Dataset,MapDatasetç­‰çš„å®ä¾‹ï¼›

controller: å¦‚ä¸Šæ‰€ç¤ºçš„æ–°å¼•å…¥çš„å‚æ•°ï¼›

uprank: æ˜¯å¦æŒ‰ç…§æ•°æ®é•¿åº¦è¿›è¡Œå‡åºæ’åˆ—ï¼Œè¿™é‡Œæœ‰ä¸‰ä¸ªå‚æ•°ï¼ŒNoneè¡¨ç¤ºä¸åšå‡åºæˆ–é™åºæ’åˆ—ï¼›Trueè¡¨ç¤ºåšå‡åºæ’åˆ—ï¼›Falseè¡¨ç¤ºåšé™åºæ’åˆ—ã€‚é»˜è®¤ä¸ºuprank=Noneï¼›

ref_index: è¿›è¡Œå‡åºæˆ–é™åºæ’åˆ—æ—¶æ‰€å‚ç…§çš„æ ·æœ¬æ•°æ®æ‰€åœ¨çš„ç»´åº¦ï¼Œé»˜è®¤ä¸ºref_index=0ï¼›

shuffleï¼šæ˜¯å¦å¯¹æ•°æ®è¿›è¡Œæ‰“ä¹±æ“ä½œã€‚æ³¨æ„ï¼Œå¦‚æœéœ€è¦å¯¹æ•°æ®è¿›è¡Œæ‰“ä¹±ï¼Œéœ€è¦uprank=Noneä¸”shuffle=True, å¦‚æœuprankä¸ä¸ºNone,åˆ™shuffleæ— æ•ˆï¼›


```python
!pip install --upgrade paddlenlp
```

    Looking in indexes: https://mirror.baidu.com/pypi/simple/
    Collecting paddlenlp
    [?25l  Downloading https://mirror.baidu.com/pypi/packages/62/10/ccc761d3e3a994703f31a4d0f93db0d13789d1c624a0cbbe9fe6439ed601/paddlenlp-2.0.5-py3-none-any.whl (435kB)
         |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 440kB 13.6MB/s eta 0:00:01
    [?25hRequirement already satisfied, skipping upgrade: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.42.1)
    Requirement already satisfied, skipping upgrade: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.2.0)
    Requirement already satisfied, skipping upgrade: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.4.4)
    Requirement already satisfied, skipping upgrade: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.1.0)
    Requirement already satisfied, skipping upgrade: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.2.2)
    Requirement already satisfied, skipping upgrade: multiprocess in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.70.11.1)
    Requirement already satisfied, skipping upgrade: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.9.0)
    Requirement already satisfied, skipping upgrade: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.8.53)
    Requirement already satisfied, skipping upgrade: Pillow>=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (7.1.2)
    Requirement already satisfied, skipping upgrade: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (2.22.0)
    Requirement already satisfied, skipping upgrade: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.7.1.1)
    Requirement already satisfied, skipping upgrade: six>=1.14.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.15.0)
    Requirement already satisfied, skipping upgrade: pandas in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.1.5)
    Requirement already satisfied, skipping upgrade: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.0.0)
    Requirement already satisfied, skipping upgrade: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.8.2)
    Requirement already satisfied, skipping upgrade: matplotlib in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (2.2.3)
    Requirement already satisfied, skipping upgrade: protobuf>=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.14.0)
    Requirement already satisfied, skipping upgrade: numpy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.20.3)
    Requirement already satisfied, skipping upgrade: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.21.0)
    Requirement already satisfied, skipping upgrade: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.1.1)
    Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (0.24.2)
    Requirement already satisfied, skipping upgrade: dill>=0.3.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from multiprocess->paddlenlp) (0.3.3)
    Requirement already satisfied, skipping upgrade: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (0.18.0)
    Requirement already satisfied, skipping upgrade: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (3.9.9)
    Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2019.9.11)
    Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (1.25.6)
    Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (3.0.4)
    Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2.8)
    Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas->visualdl->paddlenlp) (2.8.0)
    Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas->visualdl->paddlenlp) (2019.3)
    Requirement already satisfied, skipping upgrade: Jinja2>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.10.1)
    Requirement already satisfied, skipping upgrade: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.8.0)
    Requirement already satisfied, skipping upgrade: pycodestyle<2.7.0,>=2.6.0a1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.6.0)
    Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < "3.8" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (0.23)
    Requirement already satisfied, skipping upgrade: pyflakes<2.3.0,>=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.2.0)
    Requirement already satisfied, skipping upgrade: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (0.6.1)
    Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp) (2.4.2)
    Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp) (1.1.0)
    Requirement already satisfied, skipping upgrade: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp) (0.10.0)
    Requirement already satisfied, skipping upgrade: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.10.0)
    Requirement already satisfied, skipping upgrade: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.0)
    Requirement already satisfied, skipping upgrade: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.4)
    Requirement already satisfied, skipping upgrade: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (16.7.9)
    Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (5.1.2)
    Requirement already satisfied, skipping upgrade: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (2.0.1)
    Requirement already satisfied, skipping upgrade: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.4.10)
    Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (1.1.0)
    Requirement already satisfied, skipping upgrade: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (7.0)
    Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (0.16.0)
    Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.6.3)
    Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (2.1.0)
    Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (0.14.1)
    Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.5->Flask-Babel>=1.0.0->visualdl->paddlenlp) (1.1.1)
    Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < "3.8"->flake8>=3.7.9->visualdl->paddlenlp) (0.6.0)
    Requirement already satisfied, skipping upgrade: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->visualdl->paddlenlp) (56.2.0)
    Requirement already satisfied, skipping upgrade: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < "3.8"->flake8>=3.7.9->visualdl->paddlenlp) (7.2.0)
    Installing collected packages: paddlenlp
      Found existing installation: paddlenlp 2.0.1
        Uninstalling paddlenlp-2.0.1:
          Successfully uninstalled paddlenlp-2.0.1
    Successfully installed paddlenlp-2.0.5



```python
from paddle.io import Dataset, DataLoader
from paddlenlp.data import Pad, Stack, Tuple
import numpy as np


class DatasetForDBL(Dataset):
  def __init__(self, data, controller, uprank=None, ref_index=0, shuffle=False):
    super().__init__()
    self._controller = controller
    self._uprank = uprank
    self._data = data
    self._ref_index = ref_index
    self._shuffle = shuffle
    self._init_dynamic_params()

    assert isinstance(data[0][ref_index], np.ndarray), 'The input data of ref_index is supposed to be np.ndarray.'

    self._rank()
    self._update_data_index()

  def __getitem__(self, idx):
    idx = self._index_to_index[idx]
    start, end = self._index[idx]
    return [self._data[i] for i in range(start, end)]

  def __len__(self):
    return len(self._index)

  def _init_dynamic_params(self):
    self._max_length = 0
    self._batch_size = 0

  def _update_data_index(self):
    self._index = []
    batch = 0
    start_idx = 0
    for idx, sample in enumerate(self._data):
      sample_len = sample[self._ref_index].size
      self._max_length = sample_len if sample_len > self._max_length else self._max_length
      self._batch_size = int(self._controller / self._max_length)
      batch += 1
      if batch > self._batch_size:
        self._index.append((start_idx, idx))
        start_idx = idx
        self._init_dynamic_params()
        self._max_length = sample_len
        self._batch_size = int(self._controller / self._max_length**2)
        batch = 0
    self._index.append((start_idx, len(self._data)))
    self._index_to_index = list(range(len(self._index)))
    if self._shuffle:
      random.shuffle(self._index_to_index)

  def _rank(self):
    if self._uprank is None:
      if self._shuffle:
        indexes = list(range(len(self._data)))
        random.shuffle(indexes)
        self._data = [self._data[i] for i in indexes]
      return
    if self._uprank:
      self._data = sorted(self._data, key=lambda x: x[self._ref_index].size)
    else:
      self._data = sorted(self._data, key=lambda x: x[self._ref_index].size, reverse=True)
```


```python
# å®šä¹‰å‡½æ•°è¿”å›è¯»å–å™¨
def get_dynamic_batch_loader(data, controller, collate_fn, uprank=None, ref_index=0, shuffle=False, places=None):
  dataset4dbl = DatasetForDBL(data, controller, uprank, ref_index, shuffle)
  return DataLoader(dataset4dbl, batch_size=None, collate_fn=collate_fn, places=places)
```

## 2. éšæœºæ ·æœ¬æµ‹è¯•
é¦–å…ˆæˆ‘ä»¬å®šä¹‰éšæœºæ ·æœ¬ï¼Œæ¥æµ‹è¯•åŠ¨æ€BatchSizeåœ¨ä¸åŒæƒ…å¢ƒä¸‹çš„é€‚åº”æƒ…å†µã€‚è¿™é‡Œï¼Œæˆ‘ä»¬é‡‡ç”¨é¢„è®­ç»ƒçš„Bertæ¨¡å‹ï¼Œå¹¶éšæœºç”Ÿæˆä¸€ä¸ª16åˆ†ç±»çš„æ•°æ®æ ·æœ¬ï¼Œæ ·æœ¬é•¿åº¦ä»2 - 256ä¸ç­‰ï¼Œæ ·æœ¬å­—ç¬¦æ•°é‡ä¸BertTokenizerçš„ç›¸åŒï¼Œæ ·æœ¬æ•°é‡ä¸º10000ã€‚


```python
from paddle.io import Dataset
import numpy as np


class RandomDataset(Dataset):
    def __init__(self, length_range=(2, 256), 
                       num_class=16, 
                       num_sample=10000, 
                       num_token=21128,
                       for_test=False):
        super().__init__()
        self._length_range = length_range
        self._num_class = num_class
        self._num_token = num_token
        self._for_test = for_test
        self._data = [self._gen_rand_sample() for _ in range(num_sample)]
        if not for_test:
            self._y = [np.random.randint(0, self._num_class) for _ in range(num_sample)]

    def __getitem__(self, idx):
        if self._for_test:
            return np.array(self._data[idx], dtype='int64')
        else:
            y = self._y[idx]
            y = np.array(y, dtype='int64')
            return np.array(self._data[idx], dtype='int64'), y
    
    def __len__(self):
        return len(self._data)

    def _gen_rand_sample(self):
        length = np.random.randint(self._length_range[0], self._length_range[1] + 1)
        sample = np.random.random_integers(1, self._num_token - 1, (length,))
        return sample

```


```python
# çºªå½•
import visualdl

writer = visualdl.LogWriter(logdir='log')
```

### 2.0. æµ‹è¯•æƒ…æ™¯é›¶ï¼šæ­£å¸¸æƒ…å†µ
æ­£å¸¸æƒ…å†µé‡‡ç”¨å›ºå®šBatchSizeçš„æƒ…å†µè®­ç»ƒ


```python
from paddle.io import DataLoader

rand_samps = RandomDataset()
collate_fn = lambda samples, fn=Tuple(Pad(pad_val=0, axis=0), Stack()): [data for data in fn(samples)]

fx_bs_loader = DataLoader(rand_samps, batch_size=20, collate_fn=collate_fn)
```

    /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:33: DeprecationWarning: This function is deprecated. Please call randint(1, 21127 + 1) instead



```python
# è®­ç»ƒ
import paddlenlp
import paddle
import numpy as np

bert_model_0 = paddlenlp.transformers.BertForSequenceClassification.from_pretrained('bert-base-chinese', num_classes=16)

optimizer = paddle.optimizer.Adam(learning_rate=5e-3, parameters=bert_model_0.parameters())
loss_fn = paddle.nn.functional.cross_entropy

epochs = 2

train_step = 0
for epoch in range(epochs):
    for batch_id, (batch_x, batch_y) in enumerate(fx_bs_loader()):
        batch_x = paddle.to_tensor(batch_x)
        batch_y = paddle.to_tensor(batch_y)
        batch_y = paddle.unsqueeze(batch_y, axis=-1)
        out = bert_model_0(batch_x)
        # è®¡ç®—æŸå¤±
        loss = loss_fn(out, batch_y)
        # æŸå¤±åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        optimizer.clear_grad()
        # çºªå½•
        train_step += 1
        writer.add_scalar('train_loss_0', loss, train_step)
        writer.add_scalar('batch_size_0', batch_x.shape[0], train_step)
        
        if batch_id % 100 == 0:
            print('Train epoch %d, batch %d, loss: %f' % (epoch+1, train_step, loss))
```

### 2.1. æµ‹è¯•æƒ…æ™¯ä¸€ï¼šæ‰“ä¹±æƒ…å†µ
ç”±äºæ ·æœ¬æ˜¯éšæœºç”Ÿæˆçš„ï¼Œæ‰€ä»¥æ­£å¸¸æƒ…å†µä¹Ÿå³shuffle=Trueçš„æƒ…å†µï¼Œä¸¤è€…å·®è·ä¸å¤§ã€‚


```python
from paddle.io import DataLoader

rand_samps = RandomDataset()
collate_fn = lambda samples, fn=Tuple(Pad(pad_val=0, axis=0), Stack()): [data for data in fn(samples)]
controller = 5000

dy_bs_loader_1 = get_dynamic_batch_loader(rand_samps, controller, collate_fn, uprank=None)
```


```python
# è®­ç»ƒ
import paddlenlp
import paddle
import numpy as np

bert_model_1 = paddlenlp.transformers.BertForSequenceClassification.from_pretrained('bert-base-chinese', num_classes=16)

optimizer = paddle.optimizer.Adam(learning_rate=5e-3, parameters=bert_model_1.parameters())
loss_fn = paddle.nn.functional.cross_entropy

epochs = 2

train_step = 0
for epoch in range(epochs):
    for batch_id, (batch_x, batch_y) in enumerate(dy_bs_loader_1()):
        batch_x = paddle.to_tensor(batch_x)
        batch_y = paddle.to_tensor(batch_y)
        batch_y = paddle.unsqueeze(batch_y, axis=-1)
        out = bert_model_1(batch_x)
        # è®¡ç®—æŸå¤±
        loss = loss_fn(out, batch_y)
        # æŸå¤±åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        optimizer.clear_grad()
        # çºªå½•
        train_step += 1
        writer.add_scalar('train_loss_1', loss, train_step)
        writer.add_scalar('batch_size_1', batch_x.shape[0], train_step)
        
        if batch_id % 100 == 0:
            print('Train epoch %d, batch %d, loss: %f' % (epoch+1, train_step, loss))
```

### 2.2. æµ‹è¯•æƒ…æ™¯äºŒï¼šæŒ‰æ ·æœ¬é•¿åº¦æ’åº
è¿™é‡Œåªæµ‹è¯•å‡åºçš„æƒ…å†µã€‚


```python
from paddle.io import DataLoader

rand_samps = RandomDataset()
collate_fn = lambda samples, fn=Tuple(Pad(pad_val=0, axis=0), Stack()): [data for data in fn(samples)]
controller = 5000

dy_bs_loader_2 = get_dynamic_batch_loader(rand_samps, controller, collate_fn, uprank=True)
```


```python
# è®­ç»ƒ
import paddlenlp
import paddle
import numpy as np

bert_model_2 = paddlenlp.transformers.BertForSequenceClassification.from_pretrained('bert-base-chinese', num_classes=16)

optimizer = paddle.optimizer.Adam(learning_rate=5e-3, parameters=bert_model_2.parameters())
loss_fn = paddle.nn.functional.cross_entropy

epochs = 2

train_step = 0
for epoch in range(epochs):
    for batch_id, (batch_x, batch_y) in enumerate(dy_bs_loader_2()):
        batch_x = paddle.to_tensor(batch_x)
        batch_y = paddle.to_tensor(batch_y)
        batch_y = paddle.unsqueeze(batch_y, axis=-1)
        out = bert_model_2(batch_x)
        # è®¡ç®—æŸå¤±
        loss = loss_fn(out, batch_y)
        # æŸå¤±åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        optimizer.clear_grad()
        # çºªå½•
        train_step += 1
        writer.add_scalar('train_loss_2', loss, train_step)
        writer.add_scalar('batch_size_2', batch_x.shape[0], train_step)
        
        if batch_id % 100 == 0:
            print('Train epoch %d, batch %d, loss: %f' % (epoch+1, train_step, loss))
```

ç»“æœå¦‚ä¸‹ï¼š

<img src="https://ai-studio-static-online.cdn.bcebos.com/8125f21829914b9688c9e7734ad596c9dec9ba56cb804cb9977ec2b624e17640" width=400 height=400>
<img src="https://ai-studio-static-online.cdn.bcebos.com/69b1258a88874f81bf0fc8cccee0251b4a676d9fa6db42f18e5ba42e0f84ad03" width=400 height=400>

<img src="https://ai-studio-static-online.cdn.bcebos.com/c14756c08ad5456a93af11dea76513d5fc7e772723334d6187009cc147c7c156" width=400 height=400>
<img src="https://ai-studio-static-online.cdn.bcebos.com/2d3086fd88ad437ba4462f823dd105bc9ca85ff1d8904480afa88a5a974f86c9" width=400 height=400>

<img src="https://ai-studio-static-online.cdn.bcebos.com/d54de37280dc4d5ea0cf22725747d1625d4e55bd5d664036a58fe442af0b6b89" width=400 height=400>
<img src="https://ai-studio-static-online.cdn.bcebos.com/6f3e09e7608042cb8750a9e3da50a5019a33f7ae97084517bfa98f68e47cddc9" width=400 height=400>


```python

```

è§‚å¯Ÿä»¥ä¸Šç»“æœå›¾ï¼Œå¯ä»¥å¤§è‡´çœ‹å‡ºï¼ŒåŠ¨æ€BatchSizeåœ¨æ˜¾å­˜åˆ©ç”¨ç‡ä¸Šæ›´é«˜ï¼Œè®­ç»ƒæ­¥æ•°æ›´çŸ­ï¼Œé€Ÿåº¦æ›´å¿«ã€‚

## 3. çœŸå®æ•°æ®æµ‹è¯•
åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç”¨æƒ…æ„Ÿåˆ†æç›¸å…³æ•°æ®å®Œæˆåºåˆ—åˆ†ç±»ï¼ˆSequenceClassificationï¼‰æ¥æµ‹è¯•å®é™…æ•°æ®ä¸­ä¸åŒæƒ…æ™¯ä¸‹çš„è®­ç»ƒæ•ˆæœã€‚


```python
# å®šä¹‰åºåˆ—åˆ†ç±»å’Œè¯åˆ†ç±»çš„æ•°æ®é›†
from paddle.io import Dataset
from paddlenlp.data import Tuple, Pad, Stack
import paddlenlp
import random
import numpy as np


class RealDataset(Dataset):
    def __init__(self, data, label, tokenizer, max_seq_len=512, for_test=False):
        super().__init__()
        self._data = data
        self._label = label
        self._tokenizer = tokenizer
        self._max_seq_len = max_seq_len
        self._for_test = for_test
    
    def __getitem__(self, idx):
        x = self._tokenizer.encode(self._data[idx], max_seq_len=self._max_seq_len)['input_ids']
        if self._for_test:
            return np.array(x, dtype='int64')
        else:
            y = self._label[idx]
            y = np.array(y, dtype='int64')
            return np.array(x, dtype='int64'), y
    
    def __len__(self):
        return len(self._data)


real_dataset = paddlenlp.datasets.load_dataset('chnsenticorp', splits=('train'), lazy=False)

real_data = [d['text'] for d in real_dataset]
real_label = [d['label'] for d in real_dataset]

# æ‰“ä¹±
indexes = list(range(len(real_data)))
random.shuffle(indexes)
real_data = [real_data[i] for i in indexes]
real_label = [real_label[i] for i in indexes]

tokenizer = paddlenlp.transformers.BertTokenizer.from_pretrained('bert-base-chinese')
```

    [2021-07-11 23:21:07,819] [    INFO] - Found /home/aistudio/.paddlenlp/models/bert-base-chinese/bert-base-chinese-vocab.txt



```python
class DatasetForDBL(Dataset):
  def __init__(self, data, controller, uprank=None, ref_index=0, shuffle=False):
    super().__init__()
    self._controller = controller
    self._uprank = uprank
    self._data = data
    self._ref_index = ref_index
    self._shuffle = shuffle
    self._init_dynamic_params()

    assert isinstance(data[0][ref_index], np.ndarray), 'The input data of ref_index is supposed to be np.ndarray.'

    self._rank()
    self._update_data_index()

  def __getitem__(self, idx):
    idx = self._index_to_index[idx]
    start, end = self._index[idx]
    return [self._data[i] for i in range(start, end)]

  def __len__(self):
    return len(self._index)

  def _init_dynamic_params(self):
    self._max_length = 0
    self._batch_size = 0

  def _update_data_index(self):
    self._index = []
    batch = 0
    start_idx = 0
    for idx, sample in enumerate(self._data):
      sample_len = sample[self._ref_index].size
      self._max_length = sample_len if sample_len > self._max_length else self._max_length
      self._batch_size = int(self._controller / self._max_length)
      batch += 1
      if batch > self._batch_size:
        self._index.append((start_idx, idx))
        start_idx = idx
        self._init_dynamic_params()
        self._max_length = sample_len
        self._batch_size = int(self._controller / self._max_length**2)
        batch = 0
    self._index.append((start_idx, len(self._data)))
    self._index_to_index = list(range(len(self._index)))
    if self._shuffle:
      random.shuffle(self._index_to_index)

  def _rank(self):
    if self._uprank is None:
      if self._shuffle:
        indexes = list(range(len(self._data)))
        random.shuffle(indexes)
        self._data = [self._data[i] for i in indexes]
      return
    if self._uprank:
      self._data = sorted(self._data, key=lambda x: x[self._ref_index].size)
    else:
      self._data = sorted(self._data, key=lambda x: x[self._ref_index].size, reverse=True)


# å®šä¹‰å‡½æ•°è¿”å›è¯»å–å™¨
def get_dynamic_batch_loader(data, controller, collate_fn, uprank=None, ref_index=0, shuffle=False, places=None):
  dataset4dbl = DatasetForDBL(data, controller, uprank, ref_index, shuffle)
  return DataLoader(dataset4dbl, batch_size=None, collate_fn=collate_fn, places=places)
```


```python
# çºªå½•
import visualdl

writer = visualdl.LogWriter(logdir='log4real')
```

### 3.0. æ­£å¸¸æƒ…æ™¯
æ­£å¸¸æƒ…æ™¯å³ä½¿ç”¨å›ºå®šBatchSizeçš„æ•°æ®è¯»å–å™¨ã€‚


```python
from paddle.io import DataLoader

real_dataset = RealDataset(real_data, real_label, tokenizer, 512)
collate_fn = lambda samples, fn=Tuple(Pad(pad_val=0, axis=0), Stack()): [data for data in fn(samples)]

fx_bs_loader = DataLoader(real_dataset, batch_size=20, collate_fn=collate_fn)
```


```python
# è®­ç»ƒ
import paddlenlp
import paddle
import numpy as np

bert_model_0 = paddlenlp.transformers.BertForSequenceClassification.from_pretrained('bert-base-chinese', num_classes=2)

optimizer = paddle.optimizer.Adam(learning_rate=5e-6, parameters=bert_model_0.parameters())
loss_fn = paddle.nn.functional.cross_entropy
acc_fn = paddle.metric.accuracy

epochs = 2

train_step = 0
for epoch in range(epochs):
    acc = 0
    steps = 0
    for batch_id, (batch_x, batch_y) in enumerate(fx_bs_loader()):
        batch_x = paddle.to_tensor(batch_x)
        batch_y = paddle.to_tensor(batch_y)
        batch_y = paddle.unsqueeze(batch_y, axis=-1)
        out = bert_model_0(batch_x)
        # è®¡ç®—æŸå¤±
        loss = loss_fn(out, batch_y)
        # è®¡ç®—å‡†ç¡®ç‡
        tem_acc = acc_fn(out, batch_y)
        acc = (acc * steps + tem_acc * batch_x.shape[0]) / (steps + batch_x.shape[0])
        steps += batch_x.shape[0]
        # æŸå¤±åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        optimizer.clear_grad()
        # çºªå½•
        train_step += 1
        writer.add_scalar('real_train_loss_0', loss, train_step)
        writer.add_scalar('real_batch_size_0', batch_x.shape[0], train_step)
        writer.add_scalar('real_accuracy_0', acc, train_step)
        
        if batch_id % 100 == 0:
            print('Train epoch %d, batch %d, loss: %.6f, acc: %.6f' % (epoch+1, train_step, loss, acc))
```

    [2021-07-11 23:21:17,134] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/bert-base-chinese/bert-base-chinese.pdparams
    /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.
      warnings.warn(("Skip loading for {}. ".format(key) + str(err)))
    /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.
      warnings.warn(("Skip loading for {}. ".format(key) + str(err)))


    Train epoch 1, batch 1, loss: 0.715938, acc: 0.550000
    Train epoch 1, batch 101, loss: 0.260708, acc: 0.832673
    Train epoch 1, batch 201, loss: 0.143788, acc: 0.870896
    Train epoch 1, batch 301, loss: 0.302375, acc: 0.883223
    Train epoch 1, batch 401, loss: 0.085725, acc: 0.890898
    Train epoch 2, batch 481, loss: 0.277026, acc: 0.900000
    Train epoch 2, batch 581, loss: 0.249776, acc: 0.930198
    Train epoch 2, batch 681, loss: 0.104511, acc: 0.938806
    Train epoch 2, batch 781, loss: 0.125206, acc: 0.937376
    Train epoch 2, batch 881, loss: 0.036320, acc: 0.939776


### 3.1. æ‰“ä¹±æƒ…æ™¯
ä½¿ç”¨åŠ¨æ€BatchSizeï¼Œæ•°æ®æ‰“ä¹±ã€‚


```python
from paddle.io import DataLoader

real_dataset = RealDataset(real_data, real_label, tokenizer, 1024)
collate_fn = lambda samples, fn=Tuple(Pad(pad_val=0, axis=0), Stack()): [data for data in fn(samples)]
controller = 5000

dy_bs_loader_1 = get_dynamic_batch_loader(real_dataset, controller, collate_fn, uprank=None, shuffle=True)
```


```python
# è®­ç»ƒ
import paddlenlp
import paddle
import numpy as np

bert_model_1 = paddlenlp.transformers.BertForSequenceClassification.from_pretrained('bert-base-chinese', num_classes=2)
clip = paddle.nn.ClipGradByValue(400)
optimizer = paddle.optimizer.Adam(learning_rate=5e-6, parameters=bert_model_1.parameters(), grad_clip=clip)
loss_fn = paddle.nn.functional.cross_entropy
acc_fn = paddle.metric.accuracy

epochs = 2

train_step = 0
for epoch in range(epochs):
    acc = 0
    steps = 0
    for batch_id, (batch_x, batch_y) in enumerate(dy_bs_loader_1()):
        batch_x = paddle.to_tensor(batch_x)
        batch_y = paddle.to_tensor(batch_y)
        batch_y = paddle.unsqueeze(batch_y, axis=-1)
        out = bert_model_1(batch_x)
        # è®¡ç®—æŸå¤±
        loss = loss_fn(out, batch_y)
        # è®¡ç®—å‡†ç¡®ç‡
        tem_acc = acc_fn(out, batch_y)
        acc = (acc * steps + tem_acc * batch_x.shape[0]) / (steps + batch_x.shape[0])
        steps += batch_x.shape[0]
        # æŸå¤±åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        optimizer.clear_grad()
        # çºªå½•
        train_step += 1
        writer.add_scalar('real_train_loss_1', loss, train_step)
        writer.add_scalar('real_batch_size_1', batch_x.shape[0], train_step)
        writer.add_scalar('real_accuracy_1', acc, train_step)
        
        if batch_id % 100 == 0:
            print('Train epoch %d, batch %d, loss: %.6f, acc: %.6f' % (epoch+1, train_step, loss, acc))
```

    [2021-07-11 23:07:16,298] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/bert-base-chinese/bert-base-chinese.pdparams
    /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.
      warnings.warn(("Skip loading for {}. ".format(key) + str(err)))
    /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.
      warnings.warn(("Skip loading for {}. ".format(key) + str(err)))


    Train epoch 1, batch 1, loss: 0.754673, acc: 0.368421
    Train epoch 1, batch 101, loss: 0.195543, acc: 0.781171
    Train epoch 1, batch 201, loss: 0.283495, acc: 0.837791
    Train epoch 1, batch 301, loss: 0.311484, acc: 0.863916
    Train epoch 1, batch 401, loss: 0.188655, acc: 0.877811
    Train epoch 1, batch 501, loss: 0.065736, acc: 0.886324
    Train epoch 2, batch 590, loss: 0.236028, acc: 0.947368
    Train epoch 2, batch 690, loss: 0.044536, acc: 0.926605
    Train epoch 2, batch 790, loss: 0.225879, acc: 0.931559
    Train epoch 2, batch 890, loss: 0.318091, acc: 0.937891
    Train epoch 2, batch 990, loss: 0.115591, acc: 0.941912
    Train epoch 2, batch 1090, loss: 0.016826, acc: 0.943281


### 3.2. æŒ‰æ ·æœ¬é•¿åº¦æ’åº
åªæµ‹è¯•å‡åºçš„æƒ…å†µã€‚


```python
from paddle.io import DataLoader

real_dataset = RealDataset(real_data, real_label, tokenizer, 1024)
collate_fn = lambda samples, fn=Tuple(Pad(pad_val=0, axis=0), Stack()): [data for data in fn(samples)]
controller = 5000

dy_bs_loader_2 = get_dynamic_batch_loader(real_dataset, controller, collate_fn, uprank=True)
```


```python
# è®­ç»ƒ
import paddlenlp
import paddle
import numpy as np

bert_model_2 = paddlenlp.transformers.BertForSequenceClassification.from_pretrained('bert-base-chinese', num_classes=2)

clip = paddle.nn.ClipGradByValue(400)
optimizer = paddle.optimizer.Adam(learning_rate=5e-6, parameters=bert_model_2.parameters(), grad_clip=clip)
loss_fn = paddle.nn.functional.cross_entropy
acc_fn = paddle.metric.accuracy

epochs = 2

train_step = 0
for epoch in range(epochs):
    acc = 0
    steps = 0
    for batch_id, (batch_x, batch_y) in enumerate(dy_bs_loader_2()):
        batch_x = paddle.to_tensor(batch_x)
        batch_y = paddle.to_tensor(batch_y)
        batch_y = paddle.unsqueeze(batch_y, axis=-1)
        out = bert_model_2(batch_x)
        # è®¡ç®—æŸå¤±
        loss = loss_fn(out, batch_y)
        # è®¡ç®—å‡†ç¡®ç‡
        tem_acc = acc_fn(out, batch_y)
        acc = (acc * steps + tem_acc * batch_x.shape[0]) / (steps + batch_x.shape[0])
        steps += batch_x.shape[0]
        # æŸå¤±åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        optimizer.clear_grad()
        # çºªå½•
        train_step += 1
        writer.add_scalar('real_train_loss_2', loss, train_step)
        writer.add_scalar('real_batch_size_2', batch_x.shape[0], train_step)
        writer.add_scalar('real_accuracy_2', acc, train_step)
        
        if batch_id % 100 == 0:
            print('Train epoch %d, batch %d, loss: %.6f, acc: %.6f' % (epoch+1, train_step, loss, acc))
```

    [2021-07-11 23:02:16,449] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/bert-base-chinese/bert-base-chinese.pdparams
    /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.
      warnings.warn(("Skip loading for {}. ".format(key) + str(err)))
    /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.
      warnings.warn(("Skip loading for {}. ".format(key) + str(err)))


    Train epoch 1, batch 1, loss: 0.629536, acc: 0.703125
    Train epoch 1, batch 101, loss: 0.390604, acc: 0.833605
    Train epoch 1, batch 201, loss: 0.529718, acc: 0.856681
    Train epoch 2, batch 205, loss: 0.378667, acc: 0.854167
    Train epoch 2, batch 305, loss: 0.377293, acc: 0.916191
    Train epoch 2, batch 405, loss: 0.238848, acc: 0.925107


ç»“æœå¦‚ä¸‹ï¼š

<img src="https://ai-studio-static-online.cdn.bcebos.com/e9fc7f1f0b6148b9abdb5c72b97333689d63308235824887b3803d02657ad2df" width=300 height=300>
<img src="https://ai-studio-static-online.cdn.bcebos.com/b182f68c56af4b398a967a05c9f5b7e77cbd9f19420e4252b3f0730f45c58122" width=300 height=300>
<img src="https://ai-studio-static-online.cdn.bcebos.com/e788f586c3284b7ca49e9a62d34fe3a1169d0ba547b34ca0aeef11fa081d7a14" width=300 height=300>

<img src="https://ai-studio-static-online.cdn.bcebos.com/081034f7076e42f4b5b6a82025845322f90982b406cb49c5ba88042cfb0710cf" width=300 height=300>
<img src="https://ai-studio-static-online.cdn.bcebos.com/043358a574dd403f8d119bc0ea50c78496f26cdc76064c2a9cc9e927efcf0a2f" width=300 height=300>
<img src="https://ai-studio-static-online.cdn.bcebos.com/f83ed517df1e4f1bbf4666a2d08a91549d902e2fb11a4f8a88ed0b6e74e8178d" width=300 height=300>

<img src="https://ai-studio-static-online.cdn.bcebos.com/828aba60ee96489681d5bb52fa856d357a4cd8d9fe1e43dc847e3d60e3555e69" width=300 height=300>
<img src="https://ai-studio-static-online.cdn.bcebos.com/b3db6fa1872f4c2cb95e979aefd1dae9d4d5cdd954244b68a019f919f028e29e" width=300 height=300>
<img src="https://ai-studio-static-online.cdn.bcebos.com/13e3648591a048dba815e437459d58f208b49254fa024f3ca282e1a411205ed2" width=300 height=300>


## æ€»ç»“
ä»éšæœºæ ·æœ¬å’ŒçœŸå®æ ·æœ¬ä¸­ï¼Œæˆ‘ä»¬éƒ½å¯ä»¥çœ‹åˆ°ï¼ŒåŠ¨æ€BatchSizeçš„è®­ç»ƒæ­¥é•¿è¦å°äºå›ºå®šBatchSize,ä¸”è®­ç»ƒæ€»æ—¶é•¿è¾ƒçŸ­ï¼ˆçœŸå®æ ·æœ¬ä¸­éšæœºæ’åºçš„æ­¥é•¿åè€Œå¢åŠ ï¼Œæ˜¯å› ä¸ºæ ·æœ¬æœ€é•¿é•¿åº¦å¢åŠ åˆ°äº†1024ï¼Œè€Œå›ºå®šçš„é•¿åº¦æ˜¯512ï¼‰ã€‚è®­ç»ƒçš„æŸå¤±æ³¢åŠ¨ä¸å¤§ï¼ŒåŒæ—¶è®­ç»ƒçš„å‡†ç¡®ç‡ä¹Ÿæ²¡æœ‰å¤ªå¤§çš„å½±å“ã€‚å› æ­¤å¯ä»¥çœ‹å‡ºï¼Œé€‚å½“ä½¿ç”¨åŠ¨æ€BatchSizeå¯¹è®­ç»ƒé€Ÿåº¦æ˜¯æœ‰æå‡çš„ã€‚ä½†æ˜¯è¿™é‡Œéœ€è¦æå‡ºç»“æœä»¥å¤–çš„é—®é¢˜ï¼šåŠ¨æ€BatchSizeä¼šå‡ºç°è®­ç»ƒä¸ç¨³å®šçš„æƒ…å†µï¼Œå³æŸäº›æƒ…å†µä¸‹è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šå‡ºç°å‚æ•°ä¸ºNaNçš„æƒ…å†µï¼Œè¿™äº›é€šè¿‡è°ƒå°å­¦ä¹ ç‡å’Œå¢åŠ grad_clipä¼šæœ‰ä¸€å®šç¨‹åº¦çš„ç¼“è§£ï¼Œä½†æ˜¯å¹¶ä¸èƒ½ä»æ ¹æœ¬ä¸Šè§£å†³é—®é¢˜ã€‚æ‰“ä¹±æ•°æ®å¹¶é‡å¤è®­ç»ƒå¯èƒ½ä¼šä¸å†å‡ºç°æ­¤é—®é¢˜ï¼Œä½†å…·ä½“åŸå› æœ‰å¾…è¿›ä¸€æ­¥æ¢è®¨ã€‚æ­¤å¤–ï¼Œå¯¹äºå¤šå¡è®­ç»ƒçš„æ•ˆæœä¹Ÿæœ‰å¾…éªŒè¯ï¼Œæ„Ÿå…´è¶£çš„å¯ä»¥å…³æ³¨åç»­è¿›å±•ã€‚
